{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2017\n",
    "\n",
    "\n",
    "# Homework 2:  Link Analysis -- PageRank + SEO\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Tuesday, February 21, 2017 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* Explore real-world challenges of building a graph (in this case, from tweets), implement and test PageRank over this graph, and investigate factors that impact a page's rank on Google and Bing.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as YOUR_UIN_hw2.ipynb. Submit this notebook via ecampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Friday, February 24, 2017 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: PageRank (70 points)\n",
    "\n",
    "## A Twitter-Mentioning Graph\n",
    "\n",
    "In this assignment, we're going to adapt the classic PageRank approach to allow us to find not the most authoritative web pages, but rather to find significant Twitter users. So, instead of viewing the world as web pages with hyperlinks (where pages = nodes, hyperlinks = edges), we're going to construct a graph of Twitter users and their mentions of other Twitter users (so user = node, mention of another user = edge). Over this Twitter-user graph, we can apply the PageRank approach to order the users. The main idea is that a user who is mentioned by other users is more \"impactful\". \n",
    "\n",
    "Here is a toy example. Suppose you are given the following four tweets:\n",
    "\n",
    "* **userID**: diane, **text**: \"@bob Howdy!\"\n",
    "* **userID**: charlie, **text**: \"Welcome @bob and @alice!\"\n",
    "* **userID**: bob, **text**: \"Hi @charlie and @alice!\"\n",
    "* **userID**: alice, **text**: \"Howdy!\"\n",
    "\n",
    "There are four short tweets generated by four users. The @mentions between users form a directed graph with four nodes and five edges. E.g., the \"diane\" node has a directed edge to the \"bob\" node. Note that a retweet also contain the \"@\", so it should be counted as a mention as well.\n",
    "\n",
    "You should build a graph by parsing the tweets in the file we provide called *pagerank.json*.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "* The edges are binary and directed. If Bob mentions Alice once, in 10 tweets, or 10 times in one tweet, there is an edge from Bob to Alice, but there is not an edge from Alice to Bob.\n",
    "* If a user mentions herself, ignore it.\n",
    "* Correctly parsing @mentions in a tweet is error-prone. Use the entities field.\n",
    "* Later you will need to implement the PageRank algorithm on the graph you build here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here define your function for building the graph by parsing the input file of tweets\n",
    "# Insert as many cells as you want\n",
    "\n",
    "import json\n",
    "data=[] #storing the tweets in data\n",
    "with open('pagerank.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First I am creating a dictionary with key as id of person tweeting and value as address of a linked list whose nodes are id of people about whom key has tweeted\n",
    "class node(object):\n",
    "    def __init__(self,a,p = None):\n",
    "        self.value = a\n",
    "        self.pointer = p\n",
    "    def get_next(self):\n",
    "        return self.pointer\n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "    def set_next(self,p):\n",
    "        self.pointer = p\n",
    "    def set_value(self,a):\n",
    "        self.value = a\n",
    "\n",
    "class build_list(object):\n",
    "    def __init__(self,r=None):\n",
    "        self.root = r\n",
    "        self.size = 0\n",
    "    def get_size(self):     #returns the size of linked list\n",
    "        return self.size\n",
    "    def add_link(self,d):   #this inserts the id of person about whom someone is tweeting\n",
    "        new_node = node(d,self.root)\n",
    "        self.root = new_node\n",
    "        self.size += 1\n",
    "    def find(self,d):     #finds if the id of a person is already present in the linkedlist\n",
    "        this_node = self.root\n",
    "        while (this_node != None):\n",
    "            if this_node.get_value() == d:\n",
    "                return True\n",
    "            else:\n",
    "                this_node = this_node.get_next()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_to_screenname={}    #this is a dictionary with key as screen name and value as id so even if a person has more than one usernames the corresponding id will be the same\n",
    "for line in data:\n",
    "    id_user = line['user']['id']\n",
    "    screenname_user = line['user']['screen_name']\n",
    "    id_to_screenname[screenname_user] = id_user\n",
    "    user_entities = line['entities']['user_mentions']#[0]['screen_name']\n",
    "    for j in range(0,(len(user_entities))):    #extracting the names and id of persons being tweeted\n",
    "        extract_user_names = user_entities[j]['screen_name']\n",
    "        extracted_userid = user_entities[j]['id']\n",
    "        id_to_screenname[extract_user_names] = extracted_userid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#building the linked list in this cell. id_links is the dictionary whose key are id and values are addresses of linked list\n",
    "id_links={}\n",
    "user_entities=[]\n",
    "i=0\n",
    "for line in data:\n",
    "    id_user = line['user']['id']\n",
    "    screenname_user = line['user']['screen_name']\n",
    "    user_entities = line['entities']['user_mentions']\n",
    "    if id_user not in id_links:  #Checking if the person who posted the tweet is in our dict or noi\n",
    "        new_id = build_list()\n",
    "        id_links[id_user] = new_id\n",
    "    for j in range(0,(len(user_entities))): #extracting people mentioned in tweets using entities\n",
    "            extract_user_names = user_entities[j]['screen_name']\n",
    "            extracted_username_id = id_to_screenname[extract_user_names]\n",
    "            if extracted_username_id in id_links:   #checking if the extracted person has a node or not\n",
    "                if (id_links[id_user].find(extracted_username_id) == False): #This is used to check that a person is added only once even if he is mentioned more than once by a person\n",
    "                    if (extracted_username_id != id_user):  #to check the person is not tweeting about himself\n",
    "                        id_links[id_user].add_link(extracted_username_id)\n",
    "                        i = i+1\n",
    "            else:\n",
    "                new_id1 = build_list()\n",
    "                id_links[extracted_username_id] = new_id1\n",
    "                if (id_links[id_user].find(extracted_username_id) == False): #This is used to check that a person is added only once even if he is mentioned more than once by a person\n",
    "                    if (extracted_username_id != id_user):   #to check the person is not tweeting about himself\n",
    "                        id_links[id_user].add_link(extracted_username_id)  \n",
    "                        i = i+1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in graph is  16430 Number of edges in graph is 24257\n"
     ]
    }
   ],
   "source": [
    "# Call your function to print out the size of the graph, i.e., the number of nodes and edges\n",
    "# How you maintain the graph is totaly up to you\n",
    "# However, if you encounter any memory issues, we recommend you write the graph into a file, and load it later.\n",
    "jj = 0\n",
    "for idd in id_links:\n",
    "    jj = jj + id_links[idd].get_size()\n",
    "\n",
    "        \n",
    "print \"Number of nodes in graph is \", len(id_links), \"Number of edges in graph is\", jj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will not check the correctness of your graph. However, this will affect the PageRank results later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank Implementation\n",
    "\n",
    "Your program will return the top 10 users with highest PageRank scores. The **output** should be like:\n",
    "\n",
    "* user1 - score1\n",
    "* user2 - score2\n",
    "* ...\n",
    "* user10 - score10\n",
    "\n",
    "You should follow these **rules**:\n",
    "\n",
    "* Assume all nodes start out with equal probability.\n",
    "* The probability of the random surfer teleporting is 0.1 (that is, the damping factor is 0.9).\n",
    "* If a user is never mentioned and does not mention anyone, their PageRank scores should be zero. Do not include the user in the calculation.\n",
    "* It is up to you to decide when to terminate the PageRank calculation.\n",
    "* There are PageRank implementations out there on the web. Remember, your code should be **your own**.\n",
    "\n",
    "\n",
    "**Hints**:\n",
    "* If you're using the matrix style approach, you should use [numpy.matrix](https://docs.scipy.org/doc/numpy/reference/generated/numpy.matrix.html).\n",
    "* Scipy is built on top of Numpy and has support for sparse matrices. You most likely will not need to use Scipy unless you'd like to try out their sparse matrices.\n",
    "* If you choose to use Numpy (and Scipy), please make sure your Anaconda environment include their latest versions (Numpy 1.12.0; Scipy 0.18.1).\n",
    "* Test your parsing and PageRank calculations using a handful of tweets, before moving on to the entire file we provide.\n",
    "* We will evaluate the user ranks you provide as well as the quality of your code. So make sure that your code is clear and readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the termination condition in your PageRank implementation? Describe it below:\n",
    "\n",
    "*ADD YOUR INPUT HERE*\n",
    "I am comparing the top 10 scores between two successive iterations. if the difference between their scores is less than 0.000001 then the loop stops or else if it completes 100 iterations before reaching the previous said condition then also loop stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class node_pr(object):      #This node contains the user id and the number of peoples names tweeted by that id which is given by outlinks\n",
    "    def __init__(self,a,ol,p = None):\n",
    "        self.value = a     #id of people who tweeted about this person\n",
    "        self.outlinks = ol # No. of people tweeted by this node\n",
    "        self.pointer = p\n",
    "    def get_next(self):\n",
    "        return self.pointer\n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "    def get_outlinks(self):\n",
    "        return self.outlinks\n",
    "    def set_next(self,p):\n",
    "        self.pointer = p\n",
    "    def set_value(self,a):\n",
    "        self.value = a\n",
    "    def set_outlinks(self,ol):\n",
    "        self.outlinks = ol\n",
    "\n",
    "class build_list_pr(object):\n",
    "    def __init__(self,r=None):\n",
    "        self.root = r\n",
    "        self.size = 0\n",
    "    def get_size(self):     #returns the size of linked list\n",
    "        return self.size\n",
    "    def add_link(self,d,ol):   #this inserts the id of person about whom someone is tweeting\n",
    "        new_node = node_pr(d,ol,self.root)\n",
    "        self.root = new_node\n",
    "        self.size += 1\n",
    "    def find(self,d):     #finds if the id of a person is already present in the linkedlist\n",
    "        this_node = self.root\n",
    "        while (this_node != None):\n",
    "            if this_node.get_value() == d:\n",
    "                return True\n",
    "            else:\n",
    "                this_node = this_node.get_next()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#id_to_prcal contains a new dictionary with key being users id and value containing address of lists. these lists contain the userid of persons who tweeted about this person and the no.of people he tweeted about \n",
    "def Pageranker(id_links):\n",
    "    id_to_prcal={}     \n",
    "    convergence1={}   #this dictionary stores the scores of a iteration. if covergence2 holds scores of 8 iteration then convergence1 has scores of 7 iteration\n",
    "    convergence2={}   #this dictionary stores the scores of a iteration\n",
    "    for idd in id_links:\n",
    "        if idd not in id_to_prcal:\n",
    "            new_id = build_list_pr()\n",
    "            id_to_prcal[idd] = new_id\n",
    "        now = id_links[idd].root\n",
    "        while (now != None):\n",
    "            if now.get_value() not in id_to_prcal:\n",
    "                new_id1 = build_list_pr()\n",
    "                id_to_prcal[now.get_value()] = new_id1\n",
    "                id_to_prcal[now.get_value()].add_link(idd,id_links[idd].get_size())\n",
    "            else:\n",
    "                id_to_prcal[now.get_value()].add_link(idd,id_links[idd].get_size())\n",
    "            now = now.get_next()\n",
    "    id_to_pr={}  #I am creating a dictionary to store the page rank values of previous iterations\n",
    "    for iddd in id_to_prcal:\n",
    "            id_to_pr[iddd] = id_to_prcal[iddd]\n",
    "\n",
    "    id_to_pr2={}  #this stores the pr values after calculation in that iteration\n",
    "    for iddd in id_to_prcal:\n",
    "            id_to_pr2[iddd] = id_to_prcal[iddd]\n",
    "    for idd in id_to_prcal:\n",
    "        id_to_pr[idd] = 1.0/len(id_to_pr)  #initial pagerank values of all users is equal to 1/16430\n",
    "        convergence1[idd] = id_to_pr[idd]\n",
    "\n",
    "    N = len(id_to_pr2)   #No.of nodes\n",
    "    d = 0.9              #damping factor\n",
    "    constant = (0.1)/N   #(1-d)/N\n",
    "    flag = 1\n",
    "    for i in range(0,100):\n",
    "        for idd in id_to_prcal:\n",
    "            sum = 0.0\n",
    "            now = id_to_prcal[idd].root\n",
    "            while (now != None):   #PR of a user is the sum of PR's of people who tweeted about this user divided by the no.of outlinks of that particular user\n",
    "                sum = sum + float(id_to_pr[now.get_value()])/float(now.get_outlinks())\n",
    "                now = now.get_next()\n",
    "            sum = (sum * d) + constant\n",
    "            id_to_pr2[idd] = sum\n",
    "        for iddd in id_to_pr2:\n",
    "            id_to_pr[iddd] = id_to_pr2[iddd]\n",
    "        iii=0\n",
    "        for entry in sorted(id_to_pr.items(), key = lambda x: x[1], reverse=True):  #used to sort the dictionary in descending order according to values\n",
    "            convergence2[entry[0]] = entry[1]\n",
    "            if ((convergence2[entry[0]] - convergence1[entry[0]]) < 0.000001):  #difference between successive iterations \n",
    "                flag = flag * 1\n",
    "            else:\n",
    "                flag = 0\n",
    "            convergence1[entry[0]] = convergence2[entry[0]]\n",
    "            iii = iii + 1\n",
    "            if iii == 10:\n",
    "                break;\n",
    "        if (flag == 1):\n",
    "            break\n",
    "        else:\n",
    "            flag = 1\n",
    "            iii = 0\n",
    "    #print \"iii is \", iii, \"i is \",i\n",
    "    sum_square=0\n",
    "    for idd in id_to_pr:\n",
    "        sum_square = sum_square + id_to_pr[idd]*id_to_pr[idd]\n",
    "    convergence[idd] = id_to_pr[idd]   \n",
    "    sqrt_sum_square= sum_square ** (0.5)\n",
    "    #print \"sum_square is \", sum_square,\" square root is \", sqrt_sum_square\n",
    "    for idd in id_to_pr:\n",
    "        id_to_pr[idd] = id_to_pr[idd]/sqrt_sum_square\n",
    "    print \"Top 10 popular users are and their scores are \"\n",
    "    i=0  \n",
    "    for entry in sorted(id_to_pr.items(), key = lambda x: x[1], reverse=True):\n",
    "        #print entry#, id_to_pr[entry]\n",
    "        print (i+1),\": \",entry[0],\"     \",entry[1]\n",
    "        i = i+1\n",
    "        if i==10:\n",
    "            break\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 popular users are and their scores are \n",
      "1 :  158314798       0.26991862085\n",
      "2 :  181561712       0.22567202354\n",
      "3 :  209708391       0.211007923707\n",
      "4 :  72064417       0.192328101174\n",
      "5 :  105119490       0.176778657467\n",
      "6 :  14268057       0.162529194502\n",
      "7 :  379961664       0.15369485501\n",
      "8 :  391037985       0.147867588768\n",
      "9 :  153074065       0.147751152659\n",
      "10 :  313525912       0.14224206826\n"
     ]
    }
   ],
   "source": [
    "# Now let's call your function on the graph you've built. Output the results.\n",
    "\n",
    "\n",
    "Pageranker(id_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Search Engine Optimization (30 + 5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, your goal is to put on your \"[search engine optimization](https://en.wikipedia.org/wiki/Search_engine_optimization)\" hat. Your job is to create a webpage that scores highest for the query: **awcv9kjlh scwrlkjf4e** --- two terms, lower case, no quote. As of today (Feb 7, 2017), there are no hits for this query on either Google or Bing. Based on our discussions of search engine ranking algorithms, you know that several factors may impact a page's rank. Your goal is to use this knowledge to promote your own page to the top of the list.\n",
    "\n",
    "What we're doing here is a form of [SEO contest](https://en.wikipedia.org/wiki/SEO_contest). While you have great latitude in how you approach this problem, you are not allowed to engage in any unethical or illegal behavior. Please read the discussion of \"white hat\" versus \"black hat\" SEO over at [Wikipedia](https://en.wikipedia.org/wiki/Search_engine_optimization).\n",
    "\n",
    "\n",
    "**Rules of the game:**\n",
    "\n",
    "* Somewhere in the page (possibly in the non-viewable source html) you must include your name or some other way for us to identify you (e.g., your NetID, but not the UIN!).\n",
    "* Your target page may only be a TAMU student page, a page on your own webserver, a page on a standard blog platform (e.g., wordpress), or some other primarily user-controlled page\n",
    "* Your target page CAN NOT be a twitter account, a facebook page, a Yahoo Answers or similar page\n",
    "* No wikipedia vandalism\n",
    "* No yahoo/wiki answers questions\n",
    "* No comment spamming of blogs\n",
    "* If you have concerns/questions/clarifications, please post on Piazza and we will discuss\n",
    "\n",
    "For your homework turnin for this part, you should provide us the URL of your target page and a brief discussion (2-4 paragraphs) of the strategies you are using. We will issue the query and check the rankings at some undetermined time in the next couple of weeks. You might guess that major search engines take some time to discover and integrate new pages: if I were you, I'd get a target page up immediately.\n",
    "\n",
    "**Grading:**\n",
    "\n",
    "* 5 points for providing a valid URL\n",
    "* 20 points for a well-reasoned discussion of your strategy\n",
    "* 5 points for your page appearing in the search results by Google or Bing (no matter how is the ranking)\n",
    "\n",
    "** Bonus: **\n",
    "* 1 point for your page appearing in the top-20 on Google or Bing\n",
    "* 1 more point for your page appearing in the top-10 on Google or Bing\n",
    "* 1 more point for your page appearing in the top-5 on Google or Bing\n",
    "* 2 more points for your page being ranked first by Google or Bing. And, a vigorous announcement in class, and a high-five for having the top result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the URL of your page?\n",
    "\n",
    "*ADD YOUR INPUT HERE*\n",
    "http://people.tamu.edu/~karthik.273/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's your strategy? (2-4 paragraphs)\n",
    "\n",
    "*ADD YOUR INPUT HERE*\n",
    "My strategy is first I used tamu.edu domain. As it is a well known domain it gives higher priority. After that I searched about the top 100 most searched words on google, added hyperlinks to their google searches and kept all of these on my webpage. I was thinking as these are mostly searched words, the chance of my page getting also selected because of the presence of word will increase. After that, I created a question in Quora asking how to increase my rank and giving a link to my webpage to show what I did on so I get a backlink from Quora"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
